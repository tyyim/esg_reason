# CLAUDE.md - Project Guidelines

## ‚ö†Ô∏è **MANDATORY: Coding Standards**

**ALL evaluation and optimization scripts MUST follow:**
üìò **[CODING_BEST_PRACTICES.md](CODING_BEST_PRACTICES.md)**

**Required for every script**:
- ‚úÖ Checkpoint/resume mechanism (>10 min evals)
- ‚úÖ Structured logging (file + console)
- ‚úÖ Retry logic with exponential backoff
- ‚úÖ MLFlow tracking (Phase 2+)
- ‚úÖ Progress bars with tqdm
- ‚úÖ Structured JSON output
- ‚úÖ MMESGBench's exact eval_score()

**Reference**: `archive_old_project/code_old/colbert_full_dataset_evaluation.py`

---

## üéØ Project: ESG Reasoning with DSPy Optimization

Replicate MMESGBench baselines, then optimize with DSPy to improve ESG question answering accuracy.

---

## üìä Current Status (2025-10-13)

**‚úÖ Project refactored** - Old code archived to `archive_old_project/`

**Key Discoveries**:
- Dataset corrections: Microsoft 2023‚Üí2024, UN Gender, ISO14001
- **ANLS 0.5**: MMESGBench uses fuzzy matching (50% similarity threshold)
- **PRIMARY METRIC**: Answer accuracy (for MMESGBench comparison)
- **RESEARCH METRICS**: Retrieval & E2E accuracy (for our analysis)

---

## üèóÔ∏è Implementation Plan (4 Phases)

### Phase 1: MMESGBench Exact Replication
- **Goal**: ColBERT + Sentence Transformer (their exact approach)
- **Target**: ~40% answer accuracy
- **Location**: `phase1_mmesgbench_exact/` (TO BE CREATED)

### Phase 2: Qwen + PGvector Baseline
- **Goal**: Use existing Qwen embeddings + semantic search (NO ColBERT)
- **Data**: Existing PGvector (54,608 chunks) - **no re-parsing needed**
- **Target**: ~42% answer accuracy
- **Location**: `phase2_qwen_pgvector/` (TO BE CREATED)

### Phase 3a: DSPy Prompt Optimization (No Query Gen)
- **Goal**: Optimize reasoning + extraction prompts only
- **Target**: ~45% answer accuracy (+3% over Phase 2)
- **Location**: `phase3a_dspy_prompts/` (TO BE CREATED)

### Phase 3b: DSPy Query Generation
- **Goal**: Add query generation optimization
- **Target**: ~47% answer accuracy (+2% over Phase 3a)
- **Location**: `phase3b_dspy_query_gen/` (TO BE CREATED)

**Total Expected Improvement**: 40% ‚Üí 47% (+7% answer accuracy)

See `PROJECT_REFACTORING_PLAN.md` for complete details.

---

## üìÅ Project Structure

```
CC/
‚îú‚îÄ‚îÄ üìù Essential Documentation
‚îú‚îÄ‚îÄ CLAUDE.md                        # This file (quick reference)
‚îú‚îÄ‚îÄ CHANGELOG.md                     # Progress tracking
‚îú‚îÄ‚îÄ PROJECT_REFACTORING_PLAN.md      # Complete implementation plan
‚îú‚îÄ‚îÄ ANLS_EVALUATION_EXPLAINED.md     # Evaluation methodology
‚îú‚îÄ‚îÄ Research Plan.md                  # Research proposal (Notion sync)
‚îÇ
‚îú‚îÄ‚îÄ üìä Core Data
‚îú‚îÄ‚îÄ mmesgbench_dataset_corrected.json # 933 QA pairs (authoritative)
‚îú‚îÄ‚îÄ source_documents/                 # 45 ESG PDF documents
‚îÇ
‚îú‚îÄ‚îÄ üèóÔ∏è Core Infrastructure (Preserved)
‚îú‚îÄ‚îÄ src/                              # Core Python modules
‚îú‚îÄ‚îÄ MMESGBench/                       # Reference benchmark
‚îÇ
‚îú‚îÄ‚îÄ üîß Phase Implementations (TO BE CREATED)
‚îú‚îÄ‚îÄ phase1_mmesgbench_exact/         # Phase 1
‚îú‚îÄ‚îÄ phase2_qwen_pgvector/            # Phase 2
‚îú‚îÄ‚îÄ phase3a_dspy_prompts/            # Phase 3a
‚îú‚îÄ‚îÄ phase3b_dspy_query_gen/          # Phase 3b
‚îú‚îÄ‚îÄ unified_evaluator/               # Shared evaluation
‚îÇ
‚îî‚îÄ‚îÄ üóÑÔ∏è Archive
    ‚îî‚îÄ‚îÄ archive_old_project/          # All old work (for reference)
        ‚îú‚îÄ‚îÄ code_old/                  # Old Python scripts
        ‚îú‚îÄ‚îÄ results_old/               # Old JSON results
        ‚îú‚îÄ‚îÄ documentation_old/         # Old documentation
        ‚îú‚îÄ‚îÄ analysis_old/              # Old analysis
        ‚îî‚îÄ‚îÄ logs_old/                  # Old logs
```

---

## üîß Environment

```bash
cd /Users/victoryim/Local_Git/CC
conda activate esg_reasoning

# .env variables
DASHSCOPE_API_KEY=your_key
PG_URL=postgresql://user:pass@host:port/database
ESG_COLLECTION_NAME=MMESG

# Database: PostgreSQL + pgvector (54,608 chunks from 45 documents)
# Models: qwen-max (LLM), text-embedding-v4 (embeddings)
```

---

## üìä Evaluation Methodology

### PRIMARY METRIC: Answer Accuracy (MMESGBench comparison)
- Uses MMESGBench's exact `eval_score()` function
- **ANLS 0.5**: Fuzzy matching with 50% similarity threshold
- Allows typos, formatting differences (e.g., "North America" = "north america" = "Noth America")

### RESEARCH METRICS (our analysis only)
- **Retrieval Accuracy**: % questions with all evidence pages retrieved
- **E2E Accuracy**: Both retrieval AND answer correct

### Evaluation Code:
```python
from MMESGBench.src.eval.eval_score import eval_score

answer_score = eval_score(gt, pred, answer_type)
answer_correct = (answer_score >= 0.5)  # ANLS 0.5 threshold
```

See `ANLS_EVALUATION_EXPLAINED.md` for complete details.

---

## üöÄ Next Steps

1. ‚úÖ **Cleanup complete** - Old work archived
2. ‚è≥ Implement unified evaluator
3. ‚è≥ Implement Phase 1 (MMESGBench exact replication)
4. ‚è≥ Implement Phase 2 (Qwen + PGvector)
5. ‚è≥ Implement Phase 3a (DSPy prompts)
6. ‚è≥ Implement Phase 3b (DSPy query gen)
7. ‚è≥ Generate unified comparison table

---

## üìù Documentation Index

- **CLAUDE.md** (this file) - Quick project guidelines
- **CHANGELOG.md** - Historical progress tracking
- **PROJECT_REFACTORING_PLAN.md** - Complete 4-phase implementation plan
- **ANLS_EVALUATION_EXPLAINED.md** - Evaluation methodology reference
- **Research Plan.md** - Research proposal (sync with Notion occasionally)

---

## üí° Key Reminders

- **Answer accuracy** is PRIMARY metric (not retrieval or E2E)
- **ANLS 0.5** allows fuzzy matching for fair comparison
- Phase 2 uses **existing PGvector data** (no re-parsing!)
- All old code preserved in `archive_old_project/`

---

**Last Updated**: 2025-10-13 (Project refactoring and cleanup)
