# ESG Reasoning with DSPy Optimization

**Research Question**: Can DSPy prompt optimization match or exceed traditional fine-tuning (LoRA + RL) on ESG question answering with lower compute and fewer labels?

[![Dataset](https://img.shields.io/badge/Dataset-MMESGBench_933_QA-blue)](https://github.com/microsoft/Multimodal-ESG-Benchmark)
[![Status](https://img.shields.io/badge/Status-Test_Set_Complete-green)]()
[![Best](https://img.shields.io/badge/Best_Result-Hybrid_50.2%25-brightgreen)]()

---

## üéØ Quick Start

### ‚úÖ FINAL Results (654 Test Set) - CORRECTED Nov 9, 2025

| Approach  | Model | Accuracy | Change | Status |
|-----------|-------|----------|--------|--------|
| **üèÜ Hybrid (Format-Based)** | qwen2.5-7b | **50.2% (328/654)** | **+3.3%** ‚úÖ | Oct 22 |
| **Simple Baseline (1-stage)** | qwen2.5-7b | **48.5% (317/654)** | **+1.6%** ‚úÖ | Nov 9 |
| **MIPROv2** | qwen2.5-7b | 47.4% (310/654) | baseline | Nov 9 ‚úÖ |
| **Baseline (2-stage)** | qwen2.5-7b | 46.9% (307/654) | -0.5% | Nov 9 ‚úÖ |
| **GEPA** | qwen2.5-7b | 46.3% (303/654) | -1.1% | Nov 9 ‚úÖ |
| **DC-Bootstrap** | qwen2.5-7b | 43.7% (286/654) | -3.7% ‚ùå | Nov 9 |
| **DC-Cold (CU)** | qwen2.5-7b | 42.7% (279/654) | -4.7% ‚ùå | Nov 9 |
| **DC-RS** | qwen2.5-7b | 44.1% (41/93) *dev only* | -4.8% | Nov 10 ‚ö†Ô∏è |

**DC-RS Note**: DC-RS (Retrieval & Synthesis) = DC-CU in dev accuracy (44.1%) but 10x slower (43s vs 4s per question). Skipped test set due to computational cost with no benefit.

**üö® CRITICAL DISCOVERIES (Nov 9)**: 
- **Evaluation bugs affected ALL approaches** (DSPy + DC), not just DC
- **Original results were INCORRECT** due to:
  1. Null equivalence bug (treating "null" ‚â† "Not answerable")
  2. ANLS string bug (character-by-character instead of full string)
- **Re-scored with fixed evaluator** ‚Üí DSPy approaches outperform DC by 3-4%
- **Simple 1-stage baseline outperforms complex 2-stage by +1.6%!**
- **Hybrid format-based routing remains best overall!**

**Key Insight**: Simpler architectures can outperform complex multi-stage pipelines. The Simple Baseline (1-stage direct QA) beats the 2-stage (Reasoning + Extraction) approach and significantly outperforms DC's test-time learning (+5.8%).

See [`analysis/reports/COMPLETE_ERROR_ANALYSIS.md`](analysis/reports/COMPLETE_ERROR_ANALYSIS.md) for full analysis and [`results/SIMPLE_BASELINE_RESULTS_SUMMARY.md`](results/SIMPLE_BASELINE_RESULTS_SUMMARY.md) for Simple Baseline findings.

---

## üìä Full Dataset Evolution

| Date | Approach | Model | Dataset | Accuracy | Notes |
|------|----------|-------|---------|----------|-------|
| Sep 2025 | ColBERT | qwen-max | 933 questions | 40.5% (378/933) | |
| Oct 2025 | DSPy | qwen-max | 933 questions | 55.6% (519/933) | |
| Oct 19 | Baseline | qwen2.5-7b | 93 dev | ~~52.7%~~ **53.8%** | ‚ö†Ô∏è Corrected |
| Oct 19 | GEPA | qwen2.5-7b | 93 dev | ~~54.8%~~ **61.3%** | ‚ö†Ô∏è Corrected |
| **Oct 22** | **Hybrid** | **qwen2.5-7b** | **654 test** | **50.2% (328/654)** ‚úÖ | |
| Nov 9 | MIPROv2 | qwen2.5-7b | 654 test | 47.4% (310/654) | Corrected ‚úÖ |

---

## üèóÔ∏è Architecture

### RAG Pipeline
```
Question
   ‚Üì
PostgreSQL + pgvector Retrieval (top-5 chunks)
   ‚Üì
ESG Reasoning (DSPy ChainOfThought)
   ‚Üì
Answer Extraction (DSPy)
   ‚Üì
Structured Answer (Int/Float/Str/List/None)
```

### Optimization Approaches

**MIPROv2 (Teacher-Student)**:
- Teacher (qwen-max) generates optimized prompts
- Student (qwen2.5-7b) executes with those prompts
- Result: -4.3% (degraded) ‚ùå

**GEPA (Reflection-Based)**:
- qwen-max provides feedback on failures
- Evolves prompts through 32 iterations
- Result: +2.2% (improved) ‚úÖ

**Simple Baseline (Single-Stage)**:
- Direct question ‚Üí answer in one LLM call (no separate reasoning/extraction)
- Simpler architecture than 2-stage DSPy baseline
- Result: **48.5%** (test set) - **beats 2-stage by +1.6%**
- **Key insight**: Architectural simplicity can outperform complexity

**Dynamic Cheatsheet (Test-Time Learning)**:
- Model learns from past questions during evaluation
- Accumulates insights in evolving "cheatsheet"
- DC-Cold: 42.7% (empty cheatsheet, learns during test) ‚úÖ Corrected
- DC-Bootstrap: 43.7% (starts with dev cheatsheet) ‚úÖ Corrected
- **Result**: Underperforms even simple static prompts by 5.8%
- **Key insight**: Test-time learning underperforms proper prompt design

---

## üìÅ Repository Structure (Updated Nov 1, 2025)

```
CC/
‚îú‚îÄ‚îÄ README.md                          # This file - Quick overview
‚îú‚îÄ‚îÄ RESEARCH_FINDINGS.md               # Complete analysis & insights
‚îú‚îÄ‚îÄ CHANGELOG.md                       # Historical log
‚îú‚îÄ‚îÄ CLAUDE.md                          # AI collaboration guidelines
‚îú‚îÄ‚îÄ DC_NOTION_SUMMARY.md               # DC results for Notion ‚≠ê NEW
‚îú‚îÄ‚îÄ DC_TESTS_STATUS.md                 # DC test status & monitoring ‚≠ê NEW
‚îÇ
‚îú‚îÄ‚îÄ üìä results/                        # Organized prediction results
‚îÇ   ‚îú‚îÄ‚îÄ dev_set/                       # Dev set (93 questions)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_dev_predictions_20251019_130401.json (52.7%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gepa_dev_predictions_20251019_130401.json (54.8%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ miprov2_dev_predictions_20251019_130401.json (48.4%)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ complete_dev_analysis_20251019_130401.json
‚îÇ   ‚îú‚îÄ‚îÄ test_set/                      # Test set (654 questions)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_test_predictions_20251021_225632.json (47.4%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gepa_test_predictions_20251021_225632.json (45.7%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ miprov2_test_predictions_20251021_225632.json (47.6%)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ complete_test_analysis_20251021_225632.json
‚îÇ   ‚îú‚îÄ‚îÄ dc_experiments/                # DC test-time learning ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_cumulative_cold_dev_20251101_153119.json (43.0%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_cumulative_cold_test_20251101_171723.json (35.6%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_cumulative_cold_test_20251101_172109.json (34.7% bootstrap)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dev_cheatsheet_20251101.txt
‚îÇ   ‚îî‚îÄ‚îÄ analysis/                      # Analysis result files
‚îÇ       ‚îú‚îÄ‚îÄ hybrid_system_analysis_results.json
‚îÇ       ‚îú‚îÄ‚îÄ domain_knowledge_investigation.json
‚îÇ       ‚îî‚îÄ‚îÄ string_llm_evaluation_results.json
‚îÇ
‚îú‚îÄ‚îÄ üî¨ analysis/                       # Analysis scripts & reports
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                       # Python analysis scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_test_set_results.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_system_analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ investigate_domain_knowledge.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_evaluate_strings.py
‚îÇ   ‚îú‚îÄ‚îÄ reports/                       # Comprehensive markdown reports
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ COMPLETE_ERROR_ANALYSIS.md (616 lines) ‚≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HYBRID_SYSTEM_FINDINGS.md (520 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TWO_STAGE_AGENTIC_DESIGN.md (900 lines)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ STRING_LLM_EVALUATION_FINDINGS.md (341 lines)
‚îÇ   ‚îî‚îÄ‚îÄ outputs/                       # Text outputs from analyses
‚îÇ
‚îú‚îÄ‚îÄ üìù docs/                           # Additional documentation
‚îÇ   ‚îú‚îÄ‚îÄ ANLS_EVALUATION_EXPLAINED.md
‚îÇ   ‚îú‚îÄ‚îÄ MODEL_CONFIGURATION.md
‚îÇ   ‚îú‚îÄ‚îÄ CODING_BEST_PRACTICES.md
‚îÇ   ‚îú‚îÄ‚îÄ GEPA_OPTIMIZED_PROMPTS.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST_EVALUATION_STATUS.md
‚îÇ   ‚îú‚îÄ‚îÄ DC_IMPLEMENTATION_GUIDE.md     # DC step-by-step guide ‚≠ê NEW
‚îÇ   ‚îî‚îÄ‚îÄ DYNAMIC_CHEATSHEET_PLAN.md     # DC planning doc ‚≠ê NEW
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è scripts/                        # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_complete_dev_evaluation.py
‚îÇ   ‚îú‚îÄ‚îÄ run_complete_test_evaluation.py
‚îÇ   ‚îú‚îÄ‚îÄ monitor_test_evaluation.py
‚îÇ   ‚îî‚îÄ‚îÄ compare_optimizations_dev_set.py
‚îÇ
‚îú‚îÄ‚îÄ üèóÔ∏è dspy_implementation/           # Core DSPy implementation
‚îÇ   ‚îú‚îÄ‚îÄ data_splits/                   # Train/dev/test splits
‚îÇ   ‚îú‚îÄ‚îÄ optimized_modules/             # GEPA/MIPROv2 modules
‚îÇ   ‚îú‚îÄ‚îÄ dc_module/                     # Dynamic Cheatsheet ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_evaluator.py            # Evaluation with checkpointing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_wrapper.py              # DashScope SDK wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_prompts.py              # ESG-specific prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_rag_module.py           # DC + RAG integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                  # DC setup & usage
‚îÇ   ‚îú‚îÄ‚îÄ dspy_rag_enhanced.py
‚îÇ   ‚îú‚îÄ‚îÄ dspy_signatures_enhanced.py
‚îÇ   ‚îú‚îÄ‚îÄ dspy_postgres_retriever.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_baseline.py
‚îÇ   ‚îú‚îÄ‚îÄ gepa_skip_baseline.py
‚îÇ   ‚îî‚îÄ‚îÄ enhanced_miprov2_qwen7b_optimization.py
‚îÇ
‚îú‚îÄ‚îÄ üíæ Data
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mmesgbench_dataset_corrected.json  # 933 ESG questions
‚îÇ   ‚îú‚îÄ‚îÄ MMESGBench/                    # Original benchmark (external repo)
‚îÇ   ‚îú‚îÄ‚îÄ dc_repo/                       # Dynamic Cheatsheet repo (gitignored) ‚≠ê NEW
‚îÇ   ‚îú‚îÄ‚îÄ source_documents/              # Original PDFs
‚îÇ   ‚îî‚îÄ‚îÄ processed_data/                # Processed chunks
‚îÇ
‚îú‚îÄ‚îÄ üîß Core
‚îÇ   ‚îú‚îÄ‚îÄ src/                           # Utility modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py              # Corrected evaluator (central import) ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation_utils.py        # Null equivalence fix ‚≠ê NEW
‚îÇ   ‚îú‚îÄ‚îÄ configs/                       # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ logs/                          # Runtime logs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dc_evaluation/             # DC evaluation logs
‚îÇ   ‚îî‚îÄ‚îÄ cache/                         # Cache data
‚îÇ
‚îî‚îÄ‚îÄ üóÑÔ∏è archive/                       # Old/outdated files
    ‚îú‚îÄ‚îÄ old_results/                   # Old prediction files
    ‚îú‚îÄ‚îÄ old_phases/                    # Old phase directories
    ‚îú‚îÄ‚îÄ old_scripts/                   # Old scripts
    ‚îú‚îÄ‚îÄ old_docs/                      # Old documentation
    ‚îî‚îÄ‚îÄ archive_old_project/           # Historical archive
```

---

## üöÄ Running Experiments

### Baseline Evaluation
```bash
conda activate esg_reasoning
cd /Users/victoryim/Local_Git/CC

python dspy_implementation/evaluate_baseline.py \
  --model qwen2.5-7b-instruct \
  --dataset dev \
  --output baseline_dev_predictions.json
```

### GEPA Optimization
```bash
python dspy_implementation/gepa_skip_baseline.py
```

### Dynamic Cheatsheet Evaluation (COMPLETE)
```bash
# Test set - Cold start
python dspy_implementation/dc_module/dc_evaluator.py \
  --dataset test \
  --variant cumulative

# Test set - Bootstrap from dev cheatsheet
python dspy_implementation/dc_module/dc_evaluator.py \
  --dataset test \
  --variant cumulative \
  --bootstrap-cheatsheet results/dc_experiments/dc_cumulative_cold_dev_{timestamp}.json
```

**Results**: DC achieved 35.6% (cold) and 34.7% (bootstrap) on test set.
**Critical Finding**: DC scored 0% on all 107 null format questions, explaining the large performance gap vs DSPy approaches.

---

## üìä Dataset

**MMESGBench**: 933 ESG question-answer pairs from 45 corporate ESG reports

- **Total**: 933 questions
- **Location**: `data/mmesgbench_dataset_corrected.json`
- **Splits**: 186 train / 93 dev / 654 test (in `dspy_implementation/data_splits/`)
- **Chunks**: 54,608 (1024-dim embeddings, text-embedding-v4)
- **Types**: Integer, Float, String, List, None
- **Source**: [Microsoft Multimodal ESG Benchmark](https://github.com/microsoft/Multimodal-ESG-Benchmark)

### Evaluation: ANLS 0.5
All production evaluation scripts use corrected eval_score with fuzzy matching (50% similarity threshold):
```python
from src.evaluation import eval_score

answer_score = eval_score(gt, pred, answer_type)
correct = (answer_score >= 0.5)  # ANLS 0.5 threshold
```

**Bug Fix (Nov 7, 2025)**: Corrected null equivalence handling in evaluation. The original MMESGBench `eval_score()` treats "null" and "Not answerable" as different strings (ANLS string distance), causing false negatives when models correctly identify unanswerable questions. All production scripts now use `src/evaluation.py` which automatically handles null-equivalent responses ("null", "not answerable", "n/a", etc.) as semantically identical. Affected 4 out of 8 DC runs with 4-14% accuracy improvements (196 false negatives corrected). See `CHANGELOG.md` for details.

---

## üí° Key Findings

### 1. Simpler Architecture > Complex Pipeline ‚≠ê **NEW**
- **Simple 1-stage**: 48.5% (direct question ‚Üí answer)
- **2-stage Baseline**: 46.9% (reasoning ‚Üí extraction)
- **Gap**: +1.6% in favor of simplicity
- **Reason**: Each stage introduces error risk; single well-designed prompt can outperform multi-stage

### 2. Static Prompts > Test-Time Learning (Current Implementation) ‚≠ê **NEW**
- **Simple Baseline (no learning)**: 48.5%
- **DC-Cold (test-time learning)**: 42.7%
- **Gap**: +5.8% in favor of static prompts
- **Reason**: DC's cheatsheet accumulation adds noise; structured knowledge needed

### 3. GEPA Works Better Than MIPROv2 (Dev Set Only)
- **GEPA**: +7.5% improvement (dev), but overfits
- **MIPROv2**: -1.1% (dev), but competitive on test
- **Reason**: Reflection-based evolution captures domain patterns better but overfits to dev

### 4. Format-Specific Performance
| Format | Baseline | GEPA | Change |
|--------|----------|------|--------|
| **Int** | 63.2% | **73.7%** | **+10.5%** ‚úÖ |
| **Float** | 69.2% | **76.9%** | **+7.7%** ‚úÖ |
| **List** | 23.1% | **38.5%** | **+15.4%** ‚úÖ |
| **Str** | 35.3% | 29.4% | -5.9% ‚ùå |
| **null** | 92.9% | 85.7% | -7.2% ‚ùå |

**Insight**: GEPA excels at structured extraction but struggles with text and "not answerable" detection.

### 5. Cost-Performance Tradeoff
| Model | Cost ($/1K tokens) | Test Accuracy |
|-------|-------------------|---------------|
| qwen-max | $0.06 | ~69.9% (estimated) |
| Simple Baseline | $0.0006 | 48.5% |
| **Ratio** | **100x cheaper** | **69% performance** |

### 6. Prompt Length Matters
- **Simple Baseline**: ~500 characters (optimal)
- **DSPy Baseline**: 0 characters (DSPy default)
- **GEPA**: 7,749 characters (too long)
- **Trade-off**: Well-designed concise prompts > extremely long prompts

---

## üî¨ Research Contributions

### 1. Reflection > Teacher-Student for Small Models
GEPA (reflection-based) outperformed MIPROv2 (teacher-student) by 6.4% on qwen2.5-7b.

**Implication**: For 7B models, iterative reflection with feedback is more effective than using large model prompts.

### 2. Format-Specific Optimization Potential
Different answer types benefit differently from optimization:
- Structured (Int/Float/List): Large gains
- Text (Str): Degradation  
- Null: Degradation

**Implication**: Hybrid approaches with format-specific prompts may work best.

### 3. Small Dev Sets Are Noisy
93 questions ‚Üí 1 question = 1.1% change

**Need**: Test set (654 questions) validation for confidence.

---

## üö¶ Next Steps

### Immediate (This Week)
1. ‚úÖ Dev set evaluation complete
2. ‚è≥ **Run test set (654 questions)** - Validate GEPA improvement
3. ‚è≥ Statistical significance analysis
4. ‚è≥ Update Notion with findings

### Short-term (Next 2 Weeks)
1. Try GEPA-v2 with shorter prompts (<3,000 chars)
2. Format-specific optimization (separate prompts for Int/Str/null)
3. Try larger student model (qwen2.5-14b or 32b)
4. Error pattern analysis

### Long-term (Next Month)
1. Full test set comparison (all 3 approaches)
2. Compare DSPy vs fine-tuning (LoRA + RL)
3. Production deployment strategy
4. Paper preparation

---

## ü§ù Collaboration Workflow

### For Humans
1. **Check Status**: Read this README
2. **Deep Dive**: See [`RESEARCH_FINDINGS.md`](RESEARCH_FINDINGS.md)
3. **History**: See [`CHANGELOG.md`](CHANGELOG.md)
4. **Error Analysis**: See [`DEV_SET_ERROR_ANALYSIS.md`](DEV_SET_ERROR_ANALYSIS.md)
5. **Update Notion**: Sync key findings after experiments

### For AI Assistants
1. Read README (this file) for current state
2. Read RESEARCH_FINDINGS for complete context
3. Check authoritative result files (3 JSON files)
4. Update CHANGELOG after significant work
5. Create experiment logs in `logs/`

### Running New Experiments
1. Plan hypothesis and config
2. Run experiment, save predictions
3. Update results in README
4. Document findings in RESEARCH_FINDINGS
5. Log in CHANGELOG
6. Sync to Notion

---

## üìö Documentation

- **README.md** (this file) - Quick overview & current status
- **[RESEARCH_FINDINGS.md](RESEARCH_FINDINGS.md)** - Complete analysis, findings, and recommendations
- **[CHANGELOG.md](CHANGELOG.md)** - Historical log of all work
- **[DEV_SET_ERROR_ANALYSIS.md](DEV_SET_ERROR_ANALYSIS.md)** - Detailed dev set analysis
- **[Notion Research Plan](https://www.notion.so/5f2084ba49f64166b17d52aff4abc7c2)** - Research narrative

---

## üîß Environment

```bash
# Setup
conda create -n esg_reasoning python=3.10
conda activate esg_reasoning
pip install -r dspy_implementation/requirements_dspy.txt

# Environment variables (.env)
DASHSCOPE_API_KEY=your_key
PG_URL=postgresql://user:pass@host:port/database
ESG_COLLECTION_NAME=MMESG
```

**Database**: PostgreSQL 15+ with pgvector extension (54,608 chunks)

---

## üìä Current Status

**Phase**: Test set validation complete ‚úÖ  
**Best Result**: Hybrid system 50.2% (+2.6% vs MIPROv2) ‚úÖ  
**Major Discovery**: Format-based routing beats all single models ‚≠ê  
**Updated**: October 22, 2025

---

## üìñ Quick Reference

**Authoritative Result Files**:
- `results/dev_set/*_20251019_130401.json` - Dev set predictions (baseline, GEPA, MIPROv2)
- `results/test_set/*_20251021_225632.json` - Test set predictions (baseline, GEPA, MIPROv2)

**Key Analysis Reports**:
- `analysis/reports/COMPLETE_ERROR_ANALYSIS.md` ‚≠ê - Full dev + test analysis
- `analysis/reports/HYBRID_SYSTEM_FINDINGS.md` - Format-based routing (50.2%)
- `analysis/reports/TWO_STAGE_AGENTIC_DESIGN.md` - Two-stage system design (53-55% expected)

**Key Scripts**:
- Baseline eval: `dspy_implementation/evaluate_baseline.py`
- GEPA optimization: `dspy_implementation/gepa_skip_baseline.py`
- MIPROv2 optimization: `dspy_implementation/enhanced_miprov2_qwen7b_optimization.py`
- Complete evaluation: `scripts/run_complete_test_evaluation.py`
- Hybrid analysis: `analysis/scripts/hybrid_system_analysis.py`

**Evaluation**: Uses MMESGBench's `eval_score()` with ANLS 0.5 threshold + LLM validation for strings

---

## üéØ Next Steps

**Immediate (This Week)**:
1. ‚úÖ Test set validation complete
2. ‚úÖ Error analysis complete  
3. ‚úÖ Hybrid system designed
4. ‚è≥ Implement two-stage agentic system (expected 53-55%)

**Short-term (Next 2 Weeks)**:
1. Implement format-based hybrid router
2. Test two-stage system (GEPA reasoning ‚Üí MIPROv2 extraction)
3. Statistical significance testing
4. Paper draft preparation

**Long-term (Next Month)**:
1. Production deployment of hybrid system
2. Compare against fine-tuning (LoRA + RL)
3. Paper submission (ACL/EMNLP 2026)

---

**Last Updated**: October 22, 2025  
**Status**: Test set complete, hybrid system ready, two-stage design complete  
**Contact**: [GitHub](https://github.com/tyyim/esg_reason) | [Notion](https://www.notion.so/5f2084ba49f64166b17d52aff4abc7c2)
