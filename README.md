# ESG Reasoning with DSPy Optimization

**Research Question**: Can DSPy prompt optimization match or exceed traditional fine-tuning (LoRA + RL) on ESG question answering with lower compute and fewer labels?

[![Dataset](https://img.shields.io/badge/Dataset-MMESGBench_933_QA-blue)](https://github.com/microsoft/Multimodal-ESG-Benchmark)
[![Status](https://img.shields.io/badge/Status-Test_Set_Complete-green)]()
[![Best](https://img.shields.io/badge/Best_Result-Hybrid_50.2%25-brightgreen)]()

---

## üéØ Quick Start

### ‚úÖ FINAL Results (654 Test Set)

| Approach  | Model | Accuracy | Change | Status |
|-----------|-------|----------|--------|--------|
| **Baseline** | qwen2.5-7b | 47.4% (310/654) | baseline | Oct 22 |
| **MIPROv2** | qwen2.5-7b | 47.6% (311/654) | +0.2% | Oct 22 |
| **GEPA** | qwen2.5-7b | 45.7% (299/654) | -1.7% ‚ùå | Oct 22 |
| **GEPA (LLM-corrected)** | qwen2.5-7b | 47.1% (308/654) | -0.3% | Oct 22 |
| **üèÜ Hybrid (Format-Based)** | qwen2.5-7b | **50.2% (328/654)** | **+2.6%** ‚úÖ | Oct 22 |
| **DC-Cold (Test-Time Learning)** | qwen2.5-7b | 35.6% (233/654) | -11.8% ‚ùå | Nov 1 |
| **DC-Bootstrap** | qwen2.5-7b | 34.7% (227/654) | -12.7% ‚ùå | Nov 1 |

**Major Discovery**: 
- Dev set results (93 Q) didn't generalize to test set (654 Q)
- ANLS metric failed: 46.7% false negative rate on strings
- **Hybrid format-based routing beats all single models!**

See [`analysis/reports/COMPLETE_ERROR_ANALYSIS.md`](analysis/reports/COMPLETE_ERROR_ANALYSIS.md) for full analysis.

---

## üìä Full Dataset Evolution

| Date | Approach | Model | Dataset | Accuracy |
|------|----------|-------|---------|----------|
| Sep 2025 | ColBERT | qwen-max | 933 questions | 40.5% (378/933) |
| Oct 2025 | DSPy | qwen-max | 933 questions | 55.6% (519/933) |
| Oct 19 | Baseline | qwen2.5-7b | 93 dev | 52.7% (49/93) |
| Oct 19 | GEPA | qwen2.5-7b | 93 dev | 54.8% (51/93) |
| **Oct 22** | **Hybrid** | **qwen2.5-7b** | **654 test** | **50.2% (328/654)** ‚úÖ |

---

## üèóÔ∏è Architecture

### RAG Pipeline
```
Question
   ‚Üì
PostgreSQL + pgvector Retrieval (top-5 chunks)
   ‚Üì
ESG Reasoning (DSPy ChainOfThought)
   ‚Üì
Answer Extraction (DSPy)
   ‚Üì
Structured Answer (Int/Float/Str/List/None)
```

### Optimization Approaches

**MIPROv2 (Teacher-Student)**:
- Teacher (qwen-max) generates optimized prompts
- Student (qwen2.5-7b) executes with those prompts
- Result: -4.3% (degraded) ‚ùå

**GEPA (Reflection-Based)**:
- qwen-max provides feedback on failures
- Evolves prompts through 32 iterations
- Result: +2.2% (improved) ‚úÖ

**Dynamic Cheatsheet (Test-Time Learning)**:
- Model learns from past questions during evaluation
- Accumulates insights in evolving "cheatsheet"
- DC-Cold: 35.6% (empty cheatsheet, learns during test)
- DC-Bootstrap: 34.7% (starts with dev cheatsheet)
- **Critical weakness**: 0% on null format (107 questions)
- Result: Underperformed due to inability to recognize unanswerable questions

---

## üìÅ Repository Structure (Updated Nov 1, 2025)

```
CC/
‚îú‚îÄ‚îÄ README.md                          # This file - Quick overview
‚îú‚îÄ‚îÄ RESEARCH_FINDINGS.md               # Complete analysis & insights
‚îú‚îÄ‚îÄ CHANGELOG.md                       # Historical log
‚îú‚îÄ‚îÄ CLAUDE.md                          # AI collaboration guidelines
‚îú‚îÄ‚îÄ DC_NOTION_SUMMARY.md               # DC results for Notion ‚≠ê NEW
‚îú‚îÄ‚îÄ DC_TESTS_STATUS.md                 # DC test status & monitoring ‚≠ê NEW
‚îÇ
‚îú‚îÄ‚îÄ üìä results/                        # Organized prediction results
‚îÇ   ‚îú‚îÄ‚îÄ dev_set/                       # Dev set (93 questions)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_dev_predictions_20251019_130401.json (52.7%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gepa_dev_predictions_20251019_130401.json (54.8%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ miprov2_dev_predictions_20251019_130401.json (48.4%)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ complete_dev_analysis_20251019_130401.json
‚îÇ   ‚îú‚îÄ‚îÄ test_set/                      # Test set (654 questions)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_test_predictions_20251021_225632.json (47.4%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gepa_test_predictions_20251021_225632.json (45.7%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ miprov2_test_predictions_20251021_225632.json (47.6%)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ complete_test_analysis_20251021_225632.json
‚îÇ   ‚îú‚îÄ‚îÄ dc_experiments/                # DC test-time learning ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_cumulative_cold_dev_20251101_153119.json (43.0%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_cumulative_cold_test_20251101_171723.json (35.6%)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_cumulative_cold_test_20251101_172109.json (34.7% bootstrap)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dev_cheatsheet_20251101.txt
‚îÇ   ‚îî‚îÄ‚îÄ analysis/                      # Analysis result files
‚îÇ       ‚îú‚îÄ‚îÄ hybrid_system_analysis_results.json
‚îÇ       ‚îú‚îÄ‚îÄ domain_knowledge_investigation.json
‚îÇ       ‚îî‚îÄ‚îÄ string_llm_evaluation_results.json
‚îÇ
‚îú‚îÄ‚îÄ üî¨ analysis/                       # Analysis scripts & reports
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                       # Python analysis scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyze_test_set_results.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_system_analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ investigate_domain_knowledge.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_evaluate_strings.py
‚îÇ   ‚îú‚îÄ‚îÄ reports/                       # Comprehensive markdown reports
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ COMPLETE_ERROR_ANALYSIS.md (616 lines) ‚≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HYBRID_SYSTEM_FINDINGS.md (520 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TWO_STAGE_AGENTIC_DESIGN.md (900 lines)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ STRING_LLM_EVALUATION_FINDINGS.md (341 lines)
‚îÇ   ‚îî‚îÄ‚îÄ outputs/                       # Text outputs from analyses
‚îÇ
‚îú‚îÄ‚îÄ üìù docs/                           # Additional documentation
‚îÇ   ‚îú‚îÄ‚îÄ ANLS_EVALUATION_EXPLAINED.md
‚îÇ   ‚îú‚îÄ‚îÄ MODEL_CONFIGURATION.md
‚îÇ   ‚îú‚îÄ‚îÄ CODING_BEST_PRACTICES.md
‚îÇ   ‚îú‚îÄ‚îÄ GEPA_OPTIMIZED_PROMPTS.md
‚îÇ   ‚îú‚îÄ‚îÄ TEST_EVALUATION_STATUS.md
‚îÇ   ‚îú‚îÄ‚îÄ DC_IMPLEMENTATION_GUIDE.md     # DC step-by-step guide ‚≠ê NEW
‚îÇ   ‚îî‚îÄ‚îÄ DYNAMIC_CHEATSHEET_PLAN.md     # DC planning doc ‚≠ê NEW
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è scripts/                        # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_complete_dev_evaluation.py
‚îÇ   ‚îú‚îÄ‚îÄ run_complete_test_evaluation.py
‚îÇ   ‚îú‚îÄ‚îÄ monitor_test_evaluation.py
‚îÇ   ‚îî‚îÄ‚îÄ compare_optimizations_dev_set.py
‚îÇ
‚îú‚îÄ‚îÄ üèóÔ∏è dspy_implementation/           # Core DSPy implementation
‚îÇ   ‚îú‚îÄ‚îÄ data_splits/                   # Train/dev/test splits
‚îÇ   ‚îú‚îÄ‚îÄ optimized_modules/             # GEPA/MIPROv2 modules
‚îÇ   ‚îú‚îÄ‚îÄ dc_module/                     # Dynamic Cheatsheet ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_evaluator.py            # Evaluation with checkpointing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_wrapper.py              # DashScope SDK wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_prompts.py              # ESG-specific prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dc_rag_module.py           # DC + RAG integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md                  # DC setup & usage
‚îÇ   ‚îú‚îÄ‚îÄ dspy_rag_enhanced.py
‚îÇ   ‚îú‚îÄ‚îÄ dspy_signatures_enhanced.py
‚îÇ   ‚îú‚îÄ‚îÄ dspy_postgres_retriever.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_baseline.py
‚îÇ   ‚îú‚îÄ‚îÄ gepa_skip_baseline.py
‚îÇ   ‚îî‚îÄ‚îÄ enhanced_miprov2_qwen7b_optimization.py
‚îÇ
‚îú‚îÄ‚îÄ üíæ Data
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mmesgbench_dataset_corrected.json  # 933 ESG questions
‚îÇ   ‚îú‚îÄ‚îÄ MMESGBench/                    # Original benchmark (external repo)
‚îÇ   ‚îú‚îÄ‚îÄ dc_repo/                       # Dynamic Cheatsheet repo (gitignored) ‚≠ê NEW
‚îÇ   ‚îú‚îÄ‚îÄ source_documents/              # Original PDFs
‚îÇ   ‚îî‚îÄ‚îÄ processed_data/                # Processed chunks
‚îÇ
‚îú‚îÄ‚îÄ üîß Core
‚îÇ   ‚îú‚îÄ‚îÄ src/                           # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ configs/                       # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ logs/                          # Runtime logs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dc_evaluation/             # DC evaluation logs ‚≠ê NEW
‚îÇ   ‚îî‚îÄ‚îÄ cache/                         # Cache data
‚îÇ
‚îî‚îÄ‚îÄ üóÑÔ∏è archive/                       # Old/outdated files
    ‚îú‚îÄ‚îÄ old_results/                   # Old prediction files
    ‚îú‚îÄ‚îÄ old_phases/                    # Old phase directories
    ‚îú‚îÄ‚îÄ old_scripts/                   # Old scripts
    ‚îú‚îÄ‚îÄ old_docs/                      # Old documentation
    ‚îî‚îÄ‚îÄ archive_old_project/           # Historical archive
```

---

## üöÄ Running Experiments

### Baseline Evaluation
```bash
conda activate esg_reasoning
cd /Users/victoryim/Local_Git/CC

python dspy_implementation/evaluate_baseline.py \
  --model qwen2.5-7b-instruct \
  --dataset dev \
  --output baseline_dev_predictions.json
```

### GEPA Optimization
```bash
python dspy_implementation/gepa_skip_baseline.py
```

### Dynamic Cheatsheet Evaluation (COMPLETE)
```bash
# Test set - Cold start
python dspy_implementation/dc_module/dc_evaluator.py \
  --dataset test \
  --variant cumulative

# Test set - Bootstrap from dev cheatsheet
python dspy_implementation/dc_module/dc_evaluator.py \
  --dataset test \
  --variant cumulative \
  --bootstrap-cheatsheet results/dc_experiments/dc_cumulative_cold_dev_{timestamp}.json
```

**Results**: DC achieved 35.6% (cold) and 34.7% (bootstrap) on test set.
**Critical Finding**: DC scored 0% on all 107 null format questions, explaining the large performance gap vs DSPy approaches.

---

## üìä Dataset

**MMESGBench**: 933 ESG question-answer pairs from 45 corporate ESG reports

- **Total**: 933 questions
- **Location**: `data/mmesgbench_dataset_corrected.json`
- **Splits**: 186 train / 93 dev / 654 test (in `dspy_implementation/data_splits/`)
- **Chunks**: 54,608 (1024-dim embeddings, text-embedding-v4)
- **Types**: Integer, Float, String, List, None
- **Source**: [Microsoft Multimodal ESG Benchmark](https://github.com/microsoft/Multimodal-ESG-Benchmark)

### Evaluation: ANLS 0.5
Uses MMESGBench's exact `eval_score()` function with fuzzy matching (50% similarity threshold):
```python
from MMESGBench.src.eval.eval_score import eval_score

answer_score = eval_score(gt, pred, answer_type)
correct = (answer_score >= 0.5)  # ANLS 0.5 threshold
```

---

## üí° Key Findings

### 1. GEPA Works Better Than MIPROv2
- **GEPA**: +2.2% improvement
- **MIPROv2**: -4.3% degradation
- **Reason**: Reflection-based evolution captures domain patterns better

### 2. Format-Specific Performance
| Format | Baseline | GEPA | Change |
|--------|----------|------|--------|
| **Int** | 63.2% | **73.7%** | **+10.5%** ‚úÖ |
| **Float** | 69.2% | **76.9%** | **+7.7%** ‚úÖ |
| **List** | 23.1% | **38.5%** | **+15.4%** ‚úÖ |
| **Str** | 35.3% | 29.4% | -5.9% ‚ùå |
| **null** | 92.9% | 85.7% | -7.2% ‚ùå |

**Insight**: GEPA excels at structured extraction but struggles with text and "not answerable" detection.

### 3. Cost-Performance Tradeoff
| Model | Cost ($/1K tokens) | Dev Accuracy |
|-------|-------------------|--------------|
| qwen-max | $0.06 | ~69.9% |
| qwen2.5-7b (GEPA) | $0.0006 | 54.8% |
| **Ratio** | **100x cheaper** | **78% performance** |

### 4. Prompt Length Matters
- **Baseline**: 0 characters (DSPy default)
- **GEPA**: 7,749 characters
- **Trade-off**: Longer prompts help structured data but hurt text extraction

---

## üî¨ Research Contributions

### 1. Reflection > Teacher-Student for Small Models
GEPA (reflection-based) outperformed MIPROv2 (teacher-student) by 6.4% on qwen2.5-7b.

**Implication**: For 7B models, iterative reflection with feedback is more effective than using large model prompts.

### 2. Format-Specific Optimization Potential
Different answer types benefit differently from optimization:
- Structured (Int/Float/List): Large gains
- Text (Str): Degradation  
- Null: Degradation

**Implication**: Hybrid approaches with format-specific prompts may work best.

### 3. Small Dev Sets Are Noisy
93 questions ‚Üí 1 question = 1.1% change

**Need**: Test set (654 questions) validation for confidence.

---

## üö¶ Next Steps

### Immediate (This Week)
1. ‚úÖ Dev set evaluation complete
2. ‚è≥ **Run test set (654 questions)** - Validate GEPA improvement
3. ‚è≥ Statistical significance analysis
4. ‚è≥ Update Notion with findings

### Short-term (Next 2 Weeks)
1. Try GEPA-v2 with shorter prompts (<3,000 chars)
2. Format-specific optimization (separate prompts for Int/Str/null)
3. Try larger student model (qwen2.5-14b or 32b)
4. Error pattern analysis

### Long-term (Next Month)
1. Full test set comparison (all 3 approaches)
2. Compare DSPy vs fine-tuning (LoRA + RL)
3. Production deployment strategy
4. Paper preparation

---

## ü§ù Collaboration Workflow

### For Humans
1. **Check Status**: Read this README
2. **Deep Dive**: See [`RESEARCH_FINDINGS.md`](RESEARCH_FINDINGS.md)
3. **History**: See [`CHANGELOG.md`](CHANGELOG.md)
4. **Error Analysis**: See [`DEV_SET_ERROR_ANALYSIS.md`](DEV_SET_ERROR_ANALYSIS.md)
5. **Update Notion**: Sync key findings after experiments

### For AI Assistants
1. Read README (this file) for current state
2. Read RESEARCH_FINDINGS for complete context
3. Check authoritative result files (3 JSON files)
4. Update CHANGELOG after significant work
5. Create experiment logs in `logs/`

### Running New Experiments
1. Plan hypothesis and config
2. Run experiment, save predictions
3. Update results in README
4. Document findings in RESEARCH_FINDINGS
5. Log in CHANGELOG
6. Sync to Notion

---

## üìö Documentation

- **README.md** (this file) - Quick overview & current status
- **[RESEARCH_FINDINGS.md](RESEARCH_FINDINGS.md)** - Complete analysis, findings, and recommendations
- **[CHANGELOG.md](CHANGELOG.md)** - Historical log of all work
- **[DEV_SET_ERROR_ANALYSIS.md](DEV_SET_ERROR_ANALYSIS.md)** - Detailed dev set analysis
- **[Notion Research Plan](https://www.notion.so/5f2084ba49f64166b17d52aff4abc7c2)** - Research narrative

---

## üîß Environment

```bash
# Setup
conda create -n esg_reasoning python=3.10
conda activate esg_reasoning
pip install -r dspy_implementation/requirements_dspy.txt

# Environment variables (.env)
DASHSCOPE_API_KEY=your_key
PG_URL=postgresql://user:pass@host:port/database
ESG_COLLECTION_NAME=MMESG
```

**Database**: PostgreSQL 15+ with pgvector extension (54,608 chunks)

---

## üìä Current Status

**Phase**: Test set validation complete ‚úÖ  
**Best Result**: Hybrid system 50.2% (+2.6% vs MIPROv2) ‚úÖ  
**Major Discovery**: Format-based routing beats all single models ‚≠ê  
**Updated**: October 22, 2025

---

## üìñ Quick Reference

**Authoritative Result Files**:
- `results/dev_set/*_20251019_130401.json` - Dev set predictions (baseline, GEPA, MIPROv2)
- `results/test_set/*_20251021_225632.json` - Test set predictions (baseline, GEPA, MIPROv2)

**Key Analysis Reports**:
- `analysis/reports/COMPLETE_ERROR_ANALYSIS.md` ‚≠ê - Full dev + test analysis
- `analysis/reports/HYBRID_SYSTEM_FINDINGS.md` - Format-based routing (50.2%)
- `analysis/reports/TWO_STAGE_AGENTIC_DESIGN.md` - Two-stage system design (53-55% expected)

**Key Scripts**:
- Baseline eval: `dspy_implementation/evaluate_baseline.py`
- GEPA optimization: `dspy_implementation/gepa_skip_baseline.py`
- MIPROv2 optimization: `dspy_implementation/enhanced_miprov2_qwen7b_optimization.py`
- Complete evaluation: `scripts/run_complete_test_evaluation.py`
- Hybrid analysis: `analysis/scripts/hybrid_system_analysis.py`

**Evaluation**: Uses MMESGBench's `eval_score()` with ANLS 0.5 threshold + LLM validation for strings

---

## üéØ Next Steps

**Immediate (This Week)**:
1. ‚úÖ Test set validation complete
2. ‚úÖ Error analysis complete  
3. ‚úÖ Hybrid system designed
4. ‚è≥ Implement two-stage agentic system (expected 53-55%)

**Short-term (Next 2 Weeks)**:
1. Implement format-based hybrid router
2. Test two-stage system (GEPA reasoning ‚Üí MIPROv2 extraction)
3. Statistical significance testing
4. Paper draft preparation

**Long-term (Next Month)**:
1. Production deployment of hybrid system
2. Compare against fine-tuning (LoRA + RL)
3. Paper submission (ACL/EMNLP 2026)

---

**Last Updated**: October 22, 2025  
**Status**: Test set complete, hybrid system ready, two-stage design complete  
**Contact**: [GitHub](https://github.com/tyyim/esg_reason) | [Notion](https://www.notion.so/5f2084ba49f64166b17d52aff4abc7c2)
