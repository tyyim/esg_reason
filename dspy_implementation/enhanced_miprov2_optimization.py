#!/usr/bin/env python3
"""
Enhanced MIPROv2 Optimization for MMESGBench RAG

Key Improvements over previous version:
1. Query generation optimization (addresses retrieval bottleneck)
2. Separate retrieval and answer metrics
3. MLFlow experiment tracking
4. End-to-end pipeline optimization

Expected improvements:
- Retrieval accuracy: 37% ‚Üí 50-60% (+13-23%)
- End-to-end accuracy: 45% ‚Üí 48-53% (+3-8%)
"""

import sys
import json
import os
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import dspy
from dspy.teleprompt import MIPROv2
import mlflow

# Add parent directory to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
os.chdir(project_root)

from dspy_implementation.dspy_setup import setup_dspy_qwen
from dspy_implementation.dspy_dataset import MMESGBenchDataset
from dspy_implementation.dspy_rag_enhanced import EnhancedMMESGBenchRAG, BaselineMMESGBenchRAG
from dspy_implementation.dspy_metrics_enhanced import (
    mmesgbench_end_to_end_metric,
    evaluate_predictions_enhanced
)
from dspy_implementation.mlflow_tracking import DSPyMLFlowTracker, create_run_name


def evaluate_rag_with_metrics(rag_module, examples, desc: str = "Evaluation"):
    """
    Evaluate RAG module with enhanced metrics.

    Args:
        rag_module: DSPy RAG module to evaluate
        examples: List of DSPy examples
        desc: Description for progress bar

    Returns:
        Dictionary with retrieval, answer, and end-to-end metrics
    """
    predictions = []

    for example in tqdm(examples, desc=desc):
        try:
            pred = rag_module(
                question=example.question,
                doc_id=example.doc_id,
                answer_format=example.answer_format
            )
            predictions.append(pred)
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error on question: {e}")
            predictions.append(dspy.Prediction(answer="Failed"))

    # Compute enhanced metrics
    results = evaluate_predictions_enhanced(predictions, examples)

    return results, predictions


def optimize_enhanced_rag(train_set, dev_set, mlflow_tracker,
                          num_candidates: int = 10,
                          init_temperature: float = 1.0):
    """
    Optimize RAG pipeline with query generation.

    This function:
    1. Evaluates baseline (without query optimization)
    2. Evaluates enhanced baseline (with query optimization, default prompts)
    3. Runs MIPROv2 to optimize query generation + reasoning + extraction
    4. Evaluates optimized model on dev set
    5. Logs everything to MLFlow

    Args:
        train_set: Training examples (186 questions, 20%)
        dev_set: Development examples (93 questions, 10%)
        mlflow_tracker: MLFlow tracking object
        num_candidates: Number of instruction candidates
        init_temperature: Temperature for candidate generation

    Returns:
        Tuple of (optimized_rag, dev_results)
    """
    print("\n" + "=" * 80)
    print("BASELINE MIPROV2 OPTIMIZATION - Reasoning + Extraction Only")
    print("=" * 80)

    # ==================================================
    # Step 1: Evaluate True Baseline on Dev Set (for fair comparison)
    # ==================================================
    print("\nüìä Step 1: Evaluating TRUE BASELINE on dev set...")
    print("   This uses raw questions for retrieval (current approach)")
    print(f"   Dev set: {len(dev_set)} questions")

    baseline_rag = BaselineMMESGBenchRAG()

    # Evaluate on full dev set for fair comparison
    baseline_results, _ = evaluate_rag_with_metrics(
        baseline_rag,
        dev_set,
        desc="Baseline eval on dev set"
    )

    print(f"\nüìà True Baseline Results (dev set):")
    print(f"   Retrieval accuracy: {baseline_results['retrieval_accuracy']:.1%}")
    print(f"   Answer accuracy: {baseline_results['answer_accuracy']:.1%}")
    print(f"   End-to-end accuracy: {baseline_results['end_to_end_accuracy']:.1%}")

    # Log to MLFlow
    mlflow_tracker.log_baseline(
        metrics=baseline_results,
        config={
            'model': 'qwen-max',
            'temperature': 0.0,
            'retrieval': 'postgresql_pgvector',
            'embedding': 'qwen_text-embedding-v4',
            'top_k': 5,
            'query_optimization': False
        }
    )

    # ==================================================
    # Step 2: Skip Enhanced RAG - Use Baseline for Optimization
    # ==================================================
    print("\nüìä Step 2: Using BASELINE RAG for optimization...")
    print("   Optimizing only: Reasoning + Extraction prompts")
    print("   NOT optimizing: Query generation (keeping raw questions)")

    # Use baseline RAG for optimization (no query generation)
    rag_to_optimize = BaselineMMESGBenchRAG()

    print(f"\nüìà Baseline to be optimized:")
    print(f"   Retrieval accuracy: {baseline_results['retrieval_accuracy']:.1%}")
    print(f"   Answer accuracy: {baseline_results['answer_accuracy']:.1%}")
    print(f"   End-to-end accuracy: {baseline_results['end_to_end_accuracy']:.1%}")

    # ==================================================
    # Step 3: Configure and Run MIPROv2 Optimization
    # ==================================================
    print(f"\nüîß Step 3: Configuring MIPROv2 optimizer...")
    print(f"   Auto mode: light (6 trials, ~20-30 min)")
    print(f"   Temperature: {init_temperature}")
    print(f"   Metric: End-to-end accuracy (retrieval + answer)")
    print(f"   Training set: {len(train_set)} questions")

    print("\nüéØ Optimizing 2 components:")
    print("   1. ESG reasoning (optimize analysis prompts)")
    print("   2. Answer extraction (optimize extraction prompts)")

    # Log optimization config
    mlflow_tracker.log_params({
        'optimizer': 'MIPROv2',
        'auto_mode': 'light',
        'init_temperature': init_temperature,
        'train_size': len(train_set),
        'query_optimization': False  # Not optimizing query generation in this test
    })

    optimizer = MIPROv2(
        metric=mmesgbench_end_to_end_metric,  # Optimize for both retrieval + answer
        auto="light",  # Light mode: 6 trials, ~20-30 min
        verbose=True
    )

    print(f"\nüöÄ Running MIPROv2 optimization...")
    print(f"   Mode: auto='light' (6 trials, ~20-30 minutes)")
    print(f"   Training on {len(train_set)} questions (20% of dataset)")
    print(f"   Progress tracked in MLFlow\\n")

    os.makedirs("checkpoints", exist_ok=True)

    try:
        # When using auto mode, don't pass manual parameters
        # They are automatically configured by the auto setting
        optimized_rag = optimizer.compile(
            student=rag_to_optimize,
            trainset=train_set
        )

        print("\n‚úÖ MIPROv2 optimization completed!")

    except Exception as e:
        print(f"\n‚ö†Ô∏è  Optimization failed: {e}")
        print(f"   Falling back to baseline")
        optimized_rag = rag_to_optimize

    # ==================================================
    # Step 4: Evaluate Optimized Model on Dev Set
    # ==================================================
    print("\nüìä Step 4: Evaluating optimized model on dev set...")
    print(f"   Dev set size: {len(dev_set)} questions (10% of dataset)")

    dev_results, dev_predictions = evaluate_rag_with_metrics(
        optimized_rag,
        dev_set,
        desc="Dev evaluation (optimized)"
    )

    # Log to MLFlow FIRST (before printing, so it happens even if print crashes)
    print("\nüìä Logging results to MLFlow...")
    try:
        mlflow_tracker.log_final_results(dev_results)

        # Save predictions as artifact
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            predictions_data = [
                {
                    'question': ex.question,
                    'answer': pred.answer if hasattr(pred, 'answer') else None,
                    'context': pred.context if hasattr(pred, 'context') else None,
                    'doc_id': ex.doc_id,
                    'ground_truth': ex.answer,
                    'evidence_pages': ex.evidence_pages
                }
                for ex, pred in zip(dev_set, dev_predictions)
            ]
            json.dump(predictions_data, f, indent=2)
            predictions_file = f.name

        mlflow.log_artifact(predictions_file, "predictions")
        print(f"   ‚úÖ MLFlow logging complete (run: {mlflow_tracker.run_id})")
        print(f"   ‚úÖ Predictions artifact saved")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  MLFlow logging error: {e}")

    # Print results (wrapped in try-except to not crash the whole script)
    try:
        print(f"\nüìà Dev Set Results (Optimized):")
        print(f"   Retrieval accuracy: {dev_results['retrieval_accuracy']:.1%} " +
              f"({dev_results['retrieval_correct']}/{dev_results['total']})")
        print(f"   Answer accuracy: {dev_results['answer_accuracy']:.1%} " +
              f"({dev_results['answer_correct']}/{dev_results['total']})")
        print(f"   End-to-end accuracy: {dev_results['end_to_end_accuracy']:.1%} " +
              f"({dev_results['end_to_end_correct']}/{dev_results['total']})")

        # Print format breakdown
        print(f"\nüìã Format Breakdown (Dev Set):")
        for fmt, stats in dev_results['by_format'].items():
            fmt_str = str(fmt) if fmt is not None else "None"
            print(f"   {fmt_str:6s}:")
            print(f"      Retrieval: {stats['retrieval_accuracy']:6.1%} ({stats['retrieval_correct']}/{stats['total']})")
            print(f"      Answer:    {stats['answer_accuracy']:6.1%} ({stats['answer_correct']}/{stats['total']})")
            print(f"      E2E:       {stats['end_to_end_accuracy']:6.1%} ({stats['end_to_end_correct']}/{stats['total']})")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Error printing results: {e}")
        print(f"   Results still logged to MLFlow successfully!")

    # ==================================================
    # Step 5: Comparison and Analysis
    # ==================================================
    try:
        print("\n" + "=" * 80)
        print("COMPARISON: Baseline vs Optimized")
        print("=" * 80)

        print(f"\nüìä Retrieval Accuracy:")
        print(f"   Baseline (default prompts):    {baseline_results['retrieval_accuracy']:.1%}")
        print(f"   Optimized (MIPROv2):           {dev_results['retrieval_accuracy']:.1%}")
        retrieval_gain = dev_results['retrieval_accuracy'] - baseline_results['retrieval_accuracy']
        print(f"   Improvement: {retrieval_gain:+.1%}")

        print(f"\nüìä Answer Accuracy:")
        print(f"   Baseline (default prompts):    {baseline_results['answer_accuracy']:.1%}")
        print(f"   Optimized (MIPROv2):           {dev_results['answer_accuracy']:.1%}")
        answer_gain = dev_results['answer_accuracy'] - baseline_results['answer_accuracy']
        print(f"   Improvement: {answer_gain:+.1%}")

        print(f"\nüìä End-to-End Accuracy:")
        print(f"   Baseline (default prompts):    {baseline_results['end_to_end_accuracy']:.1%}")
        print(f"   Optimized (MIPROv2):           {dev_results['end_to_end_accuracy']:.1%}")
        e2e_gain = dev_results['end_to_end_accuracy'] - baseline_results['end_to_end_accuracy']
        print(f"   Improvement: {e2e_gain:+.1%}")

        # Log comparison to MLFlow
        mlflow_tracker.log_comparison(
            baseline_metrics=baseline_results,
            optimized_metrics=dev_results
        )

        # Determine success
        if e2e_gain >= 0.03:
            print(f"\n‚úÖ SUCCESS: Achieved target improvement (+3% or more)!")
        elif e2e_gain > 0:
            print(f"\n‚ö†Ô∏è  Minor improvement, consider more optimization")
        else:
            print(f"\n‚ö†Ô∏è  No improvement, may need architecture changes")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Error in comparison analysis: {e}")
        print(f"   Results already logged to MLFlow")

    return optimized_rag, dev_results, dev_predictions


def main():
    """Main execution flow."""
    print("=" * 80)
    print("BASELINE MIPROV2 OPTIMIZATION - MMESGBench RAG")
    print("=" * 80)
    print(f"\nStarted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    print(f"\nüéØ Test: Baseline Prompt Optimization")
    print(f"   Optimizing: Reasoning + Extraction prompts")
    print(f"   NOT optimizing: Query generation (testing simplified approach first)")

    # Initialize DSPy
    print(f"\nüìã Setting up DSPy environment...")
    setup_dspy_qwen()
    print(f"‚úÖ DSPy configured with Qwen Max")

    # Load dataset
    print(f"\nüìä Loading MMESGBench dataset...")
    dataset = MMESGBenchDataset()

    train_set = dataset.train_set
    dev_set = dataset.dev_set
    test_set = dataset.test_set

    print(f"\nüìà Dataset Summary:")
    print(f"   Training: {len(train_set)} questions (20%)")
    print(f"   Dev: {len(dev_set)} questions (10%)")
    print(f"   Test: {len(test_set)} questions (70%)")
    print(f"   Documents: 45/45 (100% coverage)")
    print(f"   Total chunks: 54,608")

    # Initialize MLFlow tracking
    print(f"\nüìä Initializing MLFlow tracking...")
    tracker = DSPyMLFlowTracker(experiment_name="MMESGBench_Baseline_Optimization")

    run_name = create_run_name("baseline_rag_prompt_optimization")
    tracker.start_run(
        run_name=run_name,
        tags={
            'optimizer': 'MIPROv2',
            'query_optimization': 'false',
            'phase': 'baseline_prompts_only',
            'model': 'qwen-max'
        }
    )

    # Run optimization
    try:
        optimized_rag, dev_results, dev_predictions = optimize_enhanced_rag(
            train_set=train_set,
            dev_set=dev_set,
            mlflow_tracker=tracker,
            num_candidates=10,
            init_temperature=1.0
        )

        # Save optimized module
        print(f"\nüíæ Saving optimized module...")
        os.makedirs("dspy_implementation/optimized_modules", exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        module_path = f"dspy_implementation/optimized_modules/baseline_rag_{timestamp}.json"

        try:
            optimized_rag.save(module_path)
            print(f"   Saved to: {module_path}")
            tracker.log_model_artifact({'module_path': module_path}, "baseline_rag_module")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Could not save module: {e}")

        # Save detailed results
        results_file = f"baseline_rag_results_{timestamp}.json"

        detailed_results = {
            "timestamp": datetime.now().isoformat(),
            "run_name": run_name,
            "architecture": "Baseline RAG (no query generation)",
            "optimization": {
                "method": "MIPROv2",
                "train_size": len(train_set),
                "dev_size": len(dev_set),
                "auto_mode": "light",
                "init_temperature": 1.0,
                "optimized_components": [
                    "ESGReasoning",
                    "AnswerExtraction"
                ]
            },
            "dev_results": {
                "retrieval_accuracy": dev_results['retrieval_accuracy'],
                "answer_accuracy": dev_results['answer_accuracy'],
                "end_to_end_accuracy": dev_results['end_to_end_accuracy'],
                "retrieval_correct": dev_results['retrieval_correct'],
                "answer_correct": dev_results['answer_correct'],
                "end_to_end_correct": dev_results['end_to_end_correct'],
                "total": dev_results['total'],
                "by_format": dev_results['by_format']
            }
        }

        try:
            with open(results_file, 'w') as f:
                json.dump(detailed_results, f, indent=2)

            print(f"\nüíæ Detailed results saved to: {results_file}")

            # Log detailed results artifact to MLFlow
            mlflow.log_artifact(results_file, "results")
            print(f"   ‚úÖ Results artifact logged to MLFlow")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Could not save/log detailed results: {e}")

    finally:
        # Always end MLFlow run
        tracker.end_run()

    print(f"\nCompleted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    print(f"\nüéâ Optimization complete!")
    print(f"   View MLFlow results: mlflow ui")
    print(f"   Then open: http://localhost:5000")

    return optimized_rag, dev_results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Run baseline MIPROv2 optimization (reasoning + extraction prompts only)"
    )
    parser.add_argument(
        "--num-candidates",
        type=int,
        default=10,
        help="Number of instruction candidates (default: 10)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=1.0,
        help="Initial temperature for candidates (default: 1.0)"
    )

    args = parser.parse_args()

    # Run optimization
    optimized_module, results = main()

    print("\n‚úÖ Baseline MIPROv2 optimization pipeline complete!")
    print(f"   Dev end-to-end accuracy: {results['end_to_end_accuracy']:.1%}")
    print(f"   Dev retrieval accuracy: {results['retrieval_accuracy']:.1%}")
    print(f"   Dev answer accuracy: {results['answer_accuracy']:.1%}")
    print("\n   Ready for test set evaluation!")
