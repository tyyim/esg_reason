# GEPA Optimization Configuration
# Qwen 2.5 7B Student + Qwen-Max Reflection LM

experiment:
  name: "GEPA_qwen7b_optimization"
  description: "GEPA optimization with qwen2.5-7b task model and qwen-max reflection LM"
  date: "2025-10-16"

models:
  # Task model (executes with evolved prompts)
  task_lm:
    name: "qwen2.5-7b-instruct"
    provider: "dashscope"
    api_base: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    temperature: 0.0  # Deterministic for evaluation
    max_tokens: 1024
    cost_per_1k_tokens: 0.0006  # 100x cheaper than qwen-max

  # Reflection model (analyzes failures, proposes improvements)
  reflection_lm:
    name: "qwen-max"
    provider: "dashscope"
    api_base: "https://dashscope.aliyuncs.com/compatible-mode/v1"
    temperature: 1.0  # Higher temp for creative prompt proposals
    max_tokens: 4096  # Longer for detailed failure analysis
    cost_per_1k_tokens: 0.06

optimizer:
  type: "GEPA"
  auto_mode: "light"  # Options: light, medium, heavy

  # Reflection configuration
  reflection_minibatch_size: 3  # Examples per reflection step

  # Selection strategy
  candidate_selection_strategy: "pareto"  # Options: pareto, current_best

  # Merge configuration
  use_merge: true  # Combine good components from different candidates
  max_merge_invocations: 5

  # Tracking
  track_stats: true  # Return detailed optimization statistics
  use_mlflow: true  # Log to MLFlow
  seed: 42  # For reproducibility

dataset:
  name: "MMESGBench"
  total_questions: 933
  split:
    train: 186  # 20%
    dev: 93     # 10%
    test: 654   # 70%

  # Dataset stratification
  stratification:
    - evidence_type
    - difficulty_tercile

metric:
  type: "answer_only_with_feedback"
  description: "Returns {'score': float, 'feedback': str}"

  # Scoring
  anls_threshold: 0.5  # MMESGBench standard
  perfect_score: 1.0
  failure_score: 0.0

  # Feedback components
  feedback_includes:
    - overall_outcome  # ✅/⚠️/❌ status
    - question_details  # Question, type, document
    - retrieval_analysis  # Missing/extra pages
    - answer_analysis  # Ground truth vs predicted
    - type_specific_guidance  # Format-specific tips
    - actionable_recommendations  # What to improve

expected_results:
  baseline:
    answer_accuracy: 0.548  # 54.8% (from MIPROv2 test)
    retrieval_accuracy: 0.753  # Should remain constant
    end_to_end_accuracy: 0.441

  target:
    answer_accuracy: 0.570  # ≥57.0% (+2.2% like MIPROv2)
    min_improvement: 0.022  # At least +2.2%

  runtime:
    expected_minutes: 45  # Light mode
    max_minutes: 60  # With overhead

comparison:
  baseline_optimizer: "MIPROv2_teacher_student"
  baseline_result:
    improvement: 0.022  # +2.2%
    runtime_minutes: 35

  hypothesis:
    - "GEPA ≥ MIPROv2 due to richer feedback signal"
    - "GEPA more interpretable (can inspect reflections)"
    - "GEPA potentially faster convergence"

files:
  script: "dspy_implementation/gepa_qwen7b_optimization.py"
  metric: "dspy_implementation/dspy_metrics_gepa.py"
  log: "logs/gepa_test/gepa_optimization.log"
  results: "gepa_qwen7b_results_YYYYMMDD_HHMMSS.json"
  optimized_module: "dspy_implementation/optimized_modules/gepa_qwen7b_YYYYMMDD_HHMMSS.json"

notes:
  - "First GEPA test with ESG dataset"
  - "Comparing reflective evolution vs teacher-student"
  - "Rich feedback should help reflection LM understand failures"
  - "Success = ≥+2.2% improvement (match MIPROv2)"
